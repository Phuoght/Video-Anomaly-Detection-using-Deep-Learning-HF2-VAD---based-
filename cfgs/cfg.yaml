model_paras:
  final_act: False
  nf_max: 256
  nf_start: 128
  spatial_size: 32
  img_channels: 3
  motion_channels: 2
  clip_hist: 4
  clip_pred: 1
  num_flows: 4
  feature_root: 32
  num_slots: 2000
  shrink_thres: 0.0005
  mem_usage: [False, True, True, True]
  skip_ops: ["none", "concat", "concat"]
  dropout_prob: 0.2
  # new: maximum drop_path rate used for linear schedule across transformer layers
  drop_path_rate: 0.20

device: cuda:0
dataset_base_dir: ./data
dataset_name: avenue
exp_name: avenue_ML_MemAE_SC_CVAE
ckpt_root: ./ckpt
log_root: ./log
eval_root: ./eval

ML_MemAE_SC_pretrained: ./ckpt/avenue_ML_MemAE_SC/best.pth
pretrained: False
model_savename: model.pth
logevery: 100
saveevery: 1

num_epochs: 30
batchsize: 128
lr: 0.0001
num_workers: 8
alpha: 1
intensity_loss_norm: 2

# Loss weights (legacy keys kept; these are still used)
lam_frame: 1.0
lam_kl: 1.0
lam_grad: 1.5
lam_sparsity: 0.00003
lam_recon: 1.5

# new: optional perceptual / ssim weights (0 = disabled)
lam_percep: 0.0
lam_ssim: 0.0

# new: wrapper for loss weights (optional, CombinedLoss supports either)
loss_weights:
  lam_kl: 1.0
  lam_frame: 1.0
  lam_grad: 1.5
  lam_recon: 1.5
  lam_sparsity: 0.00003
  lam_percep: 0.0
  lam_ssim: 0.0

# Training / optimization helpers
w_r: 1.0
w_p: 0.1

# new: Mixed precision & gradient clipping
use_amp: True
grad_clip: 1.0

# new: maximum drop path for transformer blocks (duplicate in model_paras for convenience)
drop_path_rate: 0.20
